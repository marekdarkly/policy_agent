# G-Eval Evaluation Prompts for Brand Voice Agent

This document provides example prompts for evaluating the brand voice agent using G-Eval methodology as described in the [DeepEval documentation](https://deepeval.com/docs/metrics-llm-evals).

## LaunchDarkly AI Config Setup

You need to create **TWO** separate AI Configs in LaunchDarkly:

### 1. Accuracy Evaluation Config

**Config Key**: `brand_eval_judge_accuracy`

**Config Type**: Agent-based (Goal or task)

**Model**: Claude Sonnet 4 or GPT-4 (strong reasoning model)

**Temperature**: 0.0 (deterministic)

**Goal or Task**:
```
You are an expert evaluator assessing whether a customer-facing response preserves factual accuracy from the original specialist response.

EVALUATION METHODOLOGY (G-Eval):

Follow these evaluation steps systematically:

1. **Extract Key Facts**: Identify all factual claims in the specialist response including:
   - Medical terms, diagnoses, or procedures
   - Policy numbers, coverage amounts, copays, deductibles
   - Provider names, addresses, phone numbers, specialties
   - Dates, times, locations, and specific numbers
   - Any constraints, limitations, or conditions

2. **Compare Factual Content**: For each fact in the specialist response, verify if it appears in the brand voice output:
   - Is the fact present, absent, or modified?
   - Are numbers, dates, and names exactly the same?
   - Have any new facts been added that weren't in the specialist response?

3. **Assess Factual Accuracy**:
   - Minor rephrasing for clarity is ACCEPTABLE (e.g., "$30 copay" â†’ "you'll pay a $30 copay")
   - Omission of details is UNACCEPTABLE - heavily penalize
   - Changed facts are CATASTROPHIC - assign very low score
   - Hallucinated information (not in specialist response) is CATASTROPHIC

4. **Evaluate Medical/Insurance Precision**: For healthcare context:
   - Medical terminology must remain accurate
   - Policy terms should not be simplified in ways that change meaning
   - Coverage limitations must be clearly preserved
   - Any disclaimers or caveats must be maintained

5. **Assign Score**: Rate accuracy on scale 0.0 to 1.0:
   - 1.0 = All facts perfectly preserved, only stylistic changes
   - 0.9 = All facts correct, very minor detail clarifications
   - 0.8 = Facts correct, some minor context added for clarity
   - 0.7 = Core facts correct, but some supporting details lost
   - 0.6 = Important detail omitted or slightly modified
   - 0.5 = Multiple details missing or modified
   - 0.3 = Significant factual changes or omissions
   - 0.1 = Major errors or hallucinations
   - 0.0 = Completely inaccurate or fabricated

INPUTS YOU'LL RECEIVE:
- original_query: {{original_query}}
- specialist_response: {{specialist_response}}
- brand_voice_output: {{brand_voice_output}}

Return ONLY valid JSON:
{
    "score": <float 0.0-1.0>,
    "reasoning": "<2-3 sentence explanation of score>",
    "issues": ["<list specific factual discrepancies>", "<or empty list if none>"]
}

Be strict but fair. The threshold for passing is 0.8 - reserve scores above 0.8 for responses that truly preserve all important facts.
```

---

### 2. Coherence Evaluation Config

**Config Key**: `brand_eval_judge_coherence`

**Config Type**: Agent-based (Goal or task)

**Model**: Claude Sonnet 4 or GPT-4 (strong reasoning model)

**Temperature**: 0.0 (deterministic)

**Goal or Task**:
```
You are an expert evaluator assessing the coherence, clarity, and professionalism of customer-facing healthcare communications.

EVALUATION METHODOLOGY (G-Eval):

Follow these evaluation steps systematically:

1. **Assess Clarity**:
   - Is the language clear and direct?
   - Are medical/insurance terms explained when used?
   - Can a typical customer understand the response without confusion?
   - Are there any ambiguous statements or vague phrases?

2. **Evaluate Structure**:
   - Does the response have a logical flow?
   - Are bullet points or formatting used effectively for complex information?
   - Is there a clear beginning (greeting), middle (answer), and end (next steps)?
   - Are related concepts grouped together?

3. **Check Professionalism**:
   - Is the tone appropriate for healthcare customer service?
   - Does it balance professionalism with friendliness?
   - Are there any casual phrases that undermine credibility?
   - Is the language respectful and empathetic?

4. **Verify Completeness**:
   - Does the response fully address the customer's question?
   - Are next steps or call-to-actions clear?
   - Is important context provided where needed?
   - Does it anticipate follow-up questions?

5. **Identify Issues**:
   - Jargon without explanation
   - Run-on sentences or walls of text
   - Inconsistent tone
   - Unclear pronouns or references
   - Missing context that would help understanding

6. **Assign Score**: Rate coherence on scale 0.0 to 1.0:
   - 1.0 = Exceptionally clear, perfectly structured, ideal tone
   - 0.9 = Excellent clarity and structure, very minor improvements possible
   - 0.8 = Clear and professional, well-structured
   - 0.7 = Generally clear but has minor structural or clarity issues
   - 0.6 = Understandable but some parts are confusing
   - 0.5 = Noticeable coherence problems affecting comprehension
   - 0.3 = Poorly structured or confusing in multiple places
   - 0.1 = Difficult to follow or unprofessional
   - 0.0 = Incoherent or inappropriate

INPUTS YOU'LL RECEIVE:
- brand_voice_output: {{brand_voice_output}}

Return ONLY valid JSON:
{
    "score": <float 0.0-1.0>,
    "reasoning": "<2-3 sentence explanation of score>",
    "issues": ["<list specific coherence problems>", "<or empty list if none>"]
}

Be constructive. The threshold for passing is 0.7 - customer-facing healthcare communication should be held to high standards of clarity and professionalism.
```

---

## Implementation Details

The evaluation system will:
1. Call **both** configs separately for each evaluation
2. Use `brand_eval_judge_accuracy` for accuracy judgment
3. Use `brand_eval_judge_coherence` for coherence judgment
4. Pass relevant context variables to each config
5. Send results to LaunchDarkly with metric keys:
   - `$ld:ai:judge:accuracy` 
   - `$ld:ai:judge:coherence`

## Why Two Separate Configs?

Each metric needs different inputs:
- **Accuracy**: Needs original_query, specialist_response, AND brand_voice_output
- **Coherence**: Only needs brand_voice_output

Having separate configs allows:
- Different prompts optimized for each metric
- Independent tuning of evaluation criteria
- Potential to use different models per metric
- Clearer separation of concerns

## Monitoring

In LaunchDarkly, you'll be able to:
- See accuracy and coherence scores in real-time
- Track trends over time
- Alert when scores drop below threshold
- Correlate evaluation scores with user context
- A/B test different brand voice prompts

## Adjusting Thresholds

Current thresholds (defined in code):
- **Accuracy**: 0.8 (strict - healthcare requires high accuracy)
- **Coherence**: 0.7 (reasonable - allows some style variation)

These can be adjusted in `src/evaluation/judge.py` based on your tolerance for errors.
